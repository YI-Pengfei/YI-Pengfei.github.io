<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://YI-Pengfei.github.io</id>
    <title>积极</title>
    <updated>2020-02-14T00:43:15.892Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://YI-Pengfei.github.io"/>
    <link rel="self" href="https://YI-Pengfei.github.io/atom.xml"/>
    <subtitle>记得戴口罩</subtitle>
    <logo>https://YI-Pengfei.github.io/images/avatar.png</logo>
    <icon>https://YI-Pengfei.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, 积极</rights>
    <entry>
        <title type="html"><![CDATA[搭建模块化神经网络的八股Tensorflow]]></title>
        <id>https://YI-Pengfei.github.io/post/da-jian-mo-kuai-hua-shen-jing-wang-luo-de-ba-gu-tensorflow</id>
        <link href="https://YI-Pengfei.github.io/post/da-jian-mo-kuai-hua-shen-jing-wang-luo-de-ba-gu-tensorflow">
        </link>
        <updated>2020-02-13T04:08:53.000Z</updated>
        <content type="html"><![CDATA[<h3 id="1-前向传播就是搭建网络设计网络结构forwardpy">1. 前向传播就是搭建网络，设计网络结构（forward.py）</h3>
<pre><code>def forward(x,regularizer):  #x是输入，regularizer是正则化权重
    #该函数完成网络结构的设计，给出输入到输出的通路
    w=
    b=
    y=
    return y

def get_weight(shape,regularizer):  #shape是w的形状，regularizer是正则化权重0.001
    w = tf.Variable(  ) #括号里写赋初值的方法，
    tf.add_to_collection('losses', tf.contrib.layers.l2_regularizer(regularizer)(w))
    if regularizer!=None:  # L2正则化
        return w

def get_bias(shape):  #shape是b的形状（就是b的个数）
    b=tf.Variable(  ) #赋初值
    return b
</code></pre>
<h3 id="2-反向传播就是训练网络优化网络参数backwardpy">2. 反向传播就是训练网络，优化网络参数（backward.py）</h3>
<pre><code>def backward():
    x=tf.placeholder(    )
    y_=tf.placeholder(    )
    y=forward.forward(x,REGULARIZER) #用forward复现网络结构
    global_step = tf.Variable(0,trainable=False) #轮数计数器定义
    if loss_method=='MSE':  # 损失函数为均方误差
        loss = tf.reduce_mean(tf.square(y-y_))
    elif loss_method=='CE':  # 损失函数为交叉熵
        ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y,
                                                            labels=tf.argmax(y_,1))
        loss = tf.reduce_mean(ce)
    # 加入正则化
    loss = loss + tf.add_n(tf.get_collection('losses'))

##指数衰减学习率 ##
    learning_rate = tf.train.exponential_decay(
        LEARNING_RATE_BASE,
        global_step,
        数据集总样本数/BATCH_SIZE,
        LEARNING_RATE_DECAY,
        staircase=True)

    train_step=tf.train.GradientDescentOptimizer(learing_rate).minimize(loss,global_step=global_step) #训练过程

##滑动平均##
    ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY,global_step) #global_step与指数衰减学习率中的公用一个
    ema_op = ema.apply(tf.trainable_variable())
    with tf.control_dependencies([train_step,ema_op]):
        train_op = tf.no_op(name='train')

    ### 实例化可还原滑动平均值的saver
#    ema = tf.train.ExponentialMovingAverage(滑动平均基数)
    ema_restore = ema.variables_to_restore()
    saver = tf.train.Saver(ema_restore)

    with tf.Session() as sess:
        init_op = tf.global_variables_initializer()
        sess.run(init_op)
        for i in range(STEPS) # 迭代轮数
            sess.run(train_step,feed_dict = {x:    ,y_:    })  #执行训练过程
            if i%轮数 ==0:  #每隔一定轮数，打印信息
                saver.save(sess, os.path.join(SAVER_PATH,str(i)),
                           global_step=global_step)
                loss_v = sess.run(loss, feed_dict={x:xs, y_:ys})
                print('After %d steps, loss is: %f'%(i, loss_v))
</code></pre>
<h3 id="main函数">main函数</h3>
<pre><code>if __name__=='__main__':
    backward()
</code></pre>
]]></content>
    </entry>
</feed>